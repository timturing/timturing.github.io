Positional Encoding used in Transformer is useful for representing the relationship of different positional words. And [A Gentle Introduction to Positional Encoding in Transformer Models, Part 1 - MachineLearningMastery.com](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/) has a pretty clear explanation of the mechanism. So we are not going to explain it in detail.



